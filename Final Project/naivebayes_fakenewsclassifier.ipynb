{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector\n",
    "\n",
    "### Alex Hedrick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data (downloaded from Kaggle):\n",
    "\n",
    "(WELFake) is a dataset of 72,134 news articles with 35,028 real and 37,106 fake news. For this, authors merged four popular news datasets (i.e. Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent over-fitting of classifiers and to provide more text data for better ML training.\n",
    "\n",
    "Published in:\n",
    "IEEE Transactions on Computational Social Systems: pp. 1-13 (doi: 10.1109/TCSS.2021.3068519).\n",
    "\n",
    "0 = fake, 1 = real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "1                                                NaN   \n",
       "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "4  SATAN 2: Russia unvelis an image of its terrif...   \n",
       "\n",
       "                                                body  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "1     Did they post their votes for Hillary already?      1  \n",
       "2   Now, most of the demonstrators gathered last ...      1  \n",
       "3  A dozen politically active pastors came here f...      0  \n",
       "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "\n",
    "# read data\n",
    "news = pd.read_csv('news_data_2_recent/WELFake_Dataset.csv', sep=',', names = ['index','title','body','label'], header = None, skiprows = 1)\n",
    "news = news.drop('index', axis=1)\n",
    "news.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed missing values in 0.03 seconds\n",
      "removed non-alphanumeric characters in 14.14 seconds\n",
      "converted to lowercase in 14.33 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>no comment is expected from barack obama membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>now most of the demonstrators gathered last n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>a dozen politically active pastors came here f...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>the rs28 sarmat missile dubbed satan 2 will re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>About Time! Christian Group Sues Amazon and SP...</td>\n",
       "      <td>all we can say on this one is it s about time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "4  SATAN 2: Russia unvelis an image of its terrif...   \n",
       "5  About Time! Christian Group Sues Amazon and SP...   \n",
       "\n",
       "                                                body  label  \n",
       "0  no comment is expected from barack obama membe...      1  \n",
       "2   now most of the demonstrators gathered last n...      1  \n",
       "3  a dozen politically active pastors came here f...     -1  \n",
       "4  the rs28 sarmat missile dubbed satan 2 will re...      1  \n",
       "5  all we can say on this one is it s about time ...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from warnings import simplefilter\n",
    "import time\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "start_time = time.time()\n",
    "\n",
    "# remove all rows with missing values\n",
    "news = news.dropna()\n",
    "print('removed missing values in ' + str(np.round(time.time() - start_time,2)) + ' seconds')\n",
    "\n",
    "# # only keep first however many rows of news\n",
    "# news = news[:15000]\n",
    "\n",
    "# define set of allowed characters\n",
    "allowed_chars = set(string.ascii_letters + string.digits + ' ')\n",
    "\n",
    "# remove non-alphanumeric characters from news\n",
    "# news['title'] = news['title'].apply(lambda x: ''.join(c for c in x if c in allowed_chars))\n",
    "news['body'] = news['body'].apply(lambda x: ''.join(c for c in x if c in allowed_chars))\n",
    "print('removed non-alphanumeric characters in ' + str(np.round(time.time() - start_time,2)) + ' seconds')\n",
    "\n",
    "# convert all news to lowercase\n",
    "# news['title'] = news['title'].str.lower()\n",
    "news['body'] = news['body'].str.lower()\n",
    "print('converted to lowercase in ' + str(np.round(time.time() - start_time,2)) + ' seconds')\n",
    "\n",
    "# replace 0 with -1 for label\n",
    "# now, -1 = fake news, 1 = real news\n",
    "news['label'] = news['label'].replace(0, -1)\n",
    "\n",
    "news.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Multinomial Features\n",
    "\n",
    "Generate multinomial features based on the number of occurrences of the most common words in 'title' and 'body' using sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 327023\n",
      "finished m = 50\n",
      "finished m = 1000\n",
      "finished m = 10000\n",
      "finished m = 30000\n",
      "finished m = 50000\n",
      "finished m = 70000\n",
      "(47929, 50)\n",
      "(23608, 50)\n"
     ]
    }
   ],
   "source": [
    "# generate multinomial features based on the number of occurrences of a set of words in the message using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "# add some extra stop words\n",
    "more_stop_words = [\"s\", \"wa\", \"u\", \"ha\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(more_stop_words)\n",
    "\n",
    "# split data into training and validation sets\n",
    "xtrain, xvali, training_labels, vali_labels = train_test_split(news['body'], news['label'], test_size=0.33, random_state=1)\n",
    "\n",
    "# return the number of unique words in all training data\n",
    "def get_unique_words_count(training_data):\n",
    "    unique_words = set()\n",
    "    for message in training_data:\n",
    "        for word in message.split():\n",
    "            unique_words.add(word)\n",
    "    return len(unique_words)\n",
    "\n",
    "print(f'number of unique words: {get_unique_words_count(xtrain)}')\n",
    "\n",
    "# look at only most frequent m words\n",
    "m = [50, 1000, 10000, 30000, 50000, 70000]\n",
    "\n",
    "# initialize lists to hold training features and vali features\n",
    "training_features = []\n",
    "vali_features = []\n",
    "\n",
    "# apply CountVectorizer to messages\n",
    "for j in m:\n",
    "    count_vectorizer = CountVectorizer(stop_words=stop_words, max_features=j)\n",
    "    # count_vectorizer = CountVectorizer(stop_words=stop_words, max_features=m)\n",
    "    # count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "    # get count features for training data\n",
    "    training_features_temp = count_vectorizer.fit_transform(xtrain)\n",
    "\n",
    "    # get count features for vali data\n",
    "    vali_features_temp = count_vectorizer.transform(xvali)\n",
    "\n",
    "    # append training features to list\n",
    "    training_features.append(training_features_temp)\n",
    "    vali_features.append(vali_features_temp)\n",
    "\n",
    "    # print time elapsed\n",
    "    print(f'finished m = {j}')\n",
    "\n",
    "# now we have a list of count features and feature names for each value of m\n",
    "print(training_features[0].shape)\n",
    "print(vali_features[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homemade Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# get sparse matrices from features\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "time_start = time.time()\n",
    "\n",
    "# convert training features to sparse matrices using csr_matrix\n",
    "training_features = [csr_matrix(x) for x in training_features]\n",
    "vali_features = [csr_matrix(x) for x in vali_features]\n",
    "\n",
    "print(f'finished in {np.round(time.time() - time_start,2)} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes classifier\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# note: data labels must be -1 and 1\n",
    "class NaiveBayes:\n",
    "    def fit(self, xtrain, ytrain):\n",
    "        # tells what the classes are\n",
    "        self.classes = {-1,1}\n",
    "\n",
    "        # initialize prior and conditional dicts\n",
    "        self.priors = {}\n",
    "        self.conditionals = {}\n",
    "\n",
    "        for i in self.classes:\n",
    "            # prior for class i\n",
    "            self.priors[i] = np.mean(ytrain == i)\n",
    "            # conditional probability of each feature given class i with Laplace smoothing\n",
    "            class_indices = np.where(ytrain == i)[0]\n",
    "            class_data = xtrain[class_indices]\n",
    "            class_counts = Counter(class_data.flatten())\n",
    "            self.conditionals[i] = {k: (v + 1) / (len(class_data) + len(class_counts))\n",
    "                                    for k, v in class_counts.items()}\n",
    "\n",
    "\n",
    "    def predict(self, xtest):\n",
    "        # calculate posterior probability for each class\n",
    "        posteriors = []\n",
    "        for i in range(len(self.classes)):\n",
    "            prior = np.log(self.class_priors[i])\n",
    "            likelihood = np.sum(np.log(self.feature_likelihoods[i]) * xtest + np.log(1 - self.feature_likelihoods[i]) * (1 - xtest), axis=1)\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # get the class with maximum posterior probability\n",
    "        pred_index = np.argmax(posteriors, axis=0)\n",
    "        pred_class = self.classes[pred_index]\n",
    "\n",
    "        # calculate confidence of prediction\n",
    "        probabilities = np.exp(posteriors)\n",
    "        pred_confidence = probabilities[pred_index] / np.sum(probabilities, axis=0)\n",
    "\n",
    "        return pred_class, pred_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "flatten not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1212\\1984468049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# train naive bayes model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model trained in '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1212\\3859871085.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, xtrain, ytrain)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mclass_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mclass_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mclass_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             self.conditionals[i] = {k: (v + 1) / (len(class_data) + len(class_counts))\n\u001b[0;32m     23\u001b[0m                                     for k, v in class_counts.items()}\n",
      "\u001b[1;32mc:\\Users\\alexh\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: flatten not found"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# train naive bayes model\n",
    "nb = NaiveBayes()\n",
    "nb.fit(training_features[1], training_labels)\n",
    "print('model trained in ' + str(np.round(time.time() - time_start,2)) + ' seconds')\n",
    "\n",
    "# Make predictions on test data\n",
    "ybayes, confidences = nb.predict(vali_features[1])\n",
    "print('predictions made in ' + str(np.round(time.time() - time_start,2)) + ' seconds')\n",
    "\n",
    "# Print results\n",
    "print(f'm = {m[1]}')\n",
    "print(classification_report(vali_labels, ybayes))\n",
    "print(\"Confidences:\", confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # print classification report\n",
    "#     print(f'm = {m[i]}')\n",
    "#     print(classification_report(vali_labels, ybayes))\n",
    "\n",
    "#     # print confusion matrix\n",
    "#     plt.figure(figsize = (4,3))\n",
    "\n",
    "#     sns.heatmap(confusion_matrix(vali_labels,ybayes), annot=True, \n",
    "#                 fmt='', cmap='Blues')\n",
    "\n",
    "#     plt.title('Confusion Matrix for Naive Bayes Classifier with m = ' + str(m[i]))\n",
    "#     plt.xlabel('Predicted Labels')\n",
    "#     plt.ylabel('Real Labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
