{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>spc_latin</th>\n",
       "      <th>root_stone</th>\n",
       "      <th>root_grate</th>\n",
       "      <th>root_other</th>\n",
       "      <th>trunk_wire</th>\n",
       "      <th>trnk_light</th>\n",
       "      <th>trnk_other</th>\n",
       "      <th>brch_light</th>\n",
       "      <th>brch_shoe</th>\n",
       "      <th>brch_other</th>\n",
       "      <th>health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>Quercus palustris</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Gleditsia triacanthos var. inermis</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Gleditsia triacanthos var. inermis</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>Gleditsia triacanthos var. inermis</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Gleditsia triacanthos var. inermis</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_dbh                           spc_latin root_stone root_grate  \\\n",
       "0        21                   Quercus palustris        Yes         No   \n",
       "1         3  Gleditsia triacanthos var. inermis         No         No   \n",
       "2        10  Gleditsia triacanthos var. inermis        Yes         No   \n",
       "3        11  Gleditsia triacanthos var. inermis         No         No   \n",
       "4        11  Gleditsia triacanthos var. inermis         No         No   \n",
       "\n",
       "  root_other trunk_wire trnk_light trnk_other brch_light brch_shoe brch_other  \\\n",
       "0         No         No         No         No         No        No         No   \n",
       "1         No         No         No         No         No        No         No   \n",
       "2         No         No         No         No         No        No         No   \n",
       "3         No         No         No         No         No        No         No   \n",
       "4         No         No         No         No         No        No         No   \n",
       "\n",
       "  health  \n",
       "0   Fair  \n",
       "1   Good  \n",
       "2   Good  \n",
       "3   Good  \n",
       "4   Good  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "trees = pd.read_csv('HW5a_nyc_trees.csv', sep=',', names = ['tree_dbh','spc_latin','root_stone','root_grate','root_other','trunk_wire','trnk_light','trnk_other','brch_light','brch_shoe',\n",
    "                                                            'brch_other','health'], header = None, skiprows = 1)\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>root_stone</th>\n",
       "      <th>root_grate</th>\n",
       "      <th>root_other</th>\n",
       "      <th>trunk_wire</th>\n",
       "      <th>trnk_light</th>\n",
       "      <th>trnk_other</th>\n",
       "      <th>brch_light</th>\n",
       "      <th>brch_shoe</th>\n",
       "      <th>brch_other</th>\n",
       "      <th>spc_latin_Gleditsia triacanthos var. inermis</th>\n",
       "      <th>spc_latin_Platanus x acerifolia</th>\n",
       "      <th>spc_latin_Pyrus calleryana</th>\n",
       "      <th>spc_latin_Quercus palustris</th>\n",
       "      <th>health_Fair</th>\n",
       "      <th>health_Good</th>\n",
       "      <th>health_Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_dbh  root_stone  root_grate  root_other  trunk_wire  trnk_light  \\\n",
       "0        21           1           0           0           0           0   \n",
       "1         3           0           0           0           0           0   \n",
       "2        10           1           0           0           0           0   \n",
       "3        11           0           0           0           0           0   \n",
       "4        11           0           0           0           0           0   \n",
       "\n",
       "   trnk_other  brch_light  brch_shoe  brch_other  \\\n",
       "0           0           0          0           0   \n",
       "1           0           0          0           0   \n",
       "2           0           0          0           0   \n",
       "3           0           0          0           0   \n",
       "4           0           0          0           0   \n",
       "\n",
       "   spc_latin_Gleditsia triacanthos var. inermis  \\\n",
       "0                                             0   \n",
       "1                                             1   \n",
       "2                                             1   \n",
       "3                                             1   \n",
       "4                                             1   \n",
       "\n",
       "   spc_latin_Platanus x acerifolia  spc_latin_Pyrus calleryana  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "\n",
       "   spc_latin_Quercus palustris  health_Fair  health_Good  health_Poor  \n",
       "0                            1            1            0            0  \n",
       "1                            0            0            1            0  \n",
       "2                            0            0            1            0  \n",
       "3                            0            0            1            0  \n",
       "4                            0            0            1            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# PART A ##################\n",
    "# transform other columns so binary values are transformed to 0 and 1\n",
    "binary_cols = np.array(['root_stone','root_grate','root_other','trunk_wire','trnk_light','trnk_other','brch_light','brch_shoe','brch_other'])\n",
    "for i in binary_cols:\n",
    "    trees[i] = trees[i].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# transform latin name and health so categorical values are one-hot encoded\n",
    "trees = pd.get_dummies(trees, columns = ['spc_latin','health'])\n",
    "trees.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 10 with 25000 training points is 84.485%\n"
     ]
    }
   ],
   "source": [
    "############# PART B ##################\n",
    "validation_data_length = 25000\n",
    "\n",
    "# split data into training and validation sets\n",
    "trees_training = trees.iloc[:-validation_data_length,:]\n",
    "trees_validation = trees.iloc[-validation_data_length:,:]\n",
    "\n",
    "# implement k nearest neighbors algorithm using euclidean distance as the distance metric\n",
    "def knn(k, training_data, validation_data, health_values_int):\n",
    "\n",
    "    # initialize empty array to store predicted values\n",
    "    predicted_values = np.zeros([validation_data.shape[0],health_values_int])\n",
    "    # loop through each row in the validation data\n",
    "    for i in range(validation_data.shape[0]):\n",
    "        # calculate euclidean distance between each row in the training data and the current row in the validation data\n",
    "        diff = training_data.iloc[:,:-health_values_int].values - validation_data.iloc[i,:-health_values_int].values\n",
    "        euclidean_distance = np.sqrt(np.sum(np.square(diff), axis=1))\n",
    "\n",
    "        # sort the euclidean distance array and get the indices of the k smallest values\n",
    "        k_smallest_indices = np.argpartition(euclidean_distance, k)[:k]\n",
    "\n",
    "        # get the health values of the k nearest neighbors\n",
    "        k_smallest_health_values = training_data.iloc[k_smallest_indices,-health_values_int:].values\n",
    "\n",
    "        # get the mode of the k_smallest_health_values array\n",
    "        test = np.sum(k_smallest_health_values, axis = 0)\n",
    "        # get index of the maximum value in the test array\n",
    "        predicted_values[i] = np.zeros(health_values_int, dtype = int)\n",
    "        predicted_values[i][np.argmax(test)] = 1\n",
    "\n",
    "    return predicted_values\n",
    "\n",
    "# test the k nearest neighbors algorithm for different values of k\n",
    "# k = [1, 10, 100]\n",
    "k = [10]\n",
    "health_values_int = 3\n",
    "for i in k:\n",
    "    for j in range(25000,25001,25000):\n",
    "        predicted_values = knn(i, trees_training.iloc[:j,:], trees_validation, health_values_int)\n",
    "        print('Accuracy for k = ' + str(i) + ' with ' + str(j) + ' training points is ' + str(np.round(100*np.sum(predicted_values == trees_validation.iloc[:,-3:].values)/(health_values_int*trees_validation.shape[0]),3)) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for k = 10 with 25000 training points is:\n",
      "[[    6.    12.     6.]\n",
      " [  143.   352.   671.]\n",
      " [  757.  4229. 18824.]]\n"
     ]
    }
   ],
   "source": [
    "# Provide \"confusion matrix\" for the best k and n\n",
    "k = 10\n",
    "# n = 200000\n",
    "n = 25000\n",
    "# predicted_values = knn(k, trees_training.iloc[:n,:], trees_validation, health_values_int)\n",
    "print('Confusion matrix for k = ' + str(k) + ' with ' + str(n) + ' training points is:')\n",
    "\n",
    "confusion = np.zeros([health_values_int,health_values_int])\n",
    "k = 0\n",
    "l = 0\n",
    "for i in [2,0,1]:\n",
    "    for j in [2,0,1]:\n",
    "        confusion[k,l] = np.sum((predicted_values[:,i] == 1) & (trees_validation.iloc[:,-health_values_int:].values[:,j] == 1))\n",
    "        l += 1\n",
    "    l = 0\n",
    "    k += 1\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B Comments\n",
    "With a 25000 length validation dataset, my algorithm takes almost 2 minutes to run against just 25000 training points (I comment on possible ways to improve the speed of the algorithm after part C). Therefore, I tested the algorithm with only 2000 validation points against different sizes of training data and different values of k. I determined that k = 10 was one of the better k values, so I ran the algorithm with k = 10 for all 25000 validation points. Part of the reason the algorithm has a fairly high accuracy of ~85% is just because most of the trees have 'good' health, as is shown by the results of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 1 with 25000 training points is 66.012%\n",
      "Accuracy for k = 1 with 50000 training points is 68.664%\n",
      "Accuracy for k = 1 with 75000 training points is 66.784%\n",
      "Accuracy for k = 1 with 100000 training points is 67.156%\n",
      "Accuracy for k = 1 with 125000 training points is 65.788%\n",
      "Accuracy for k = 1 with 150000 training points is 68.848%\n",
      "Accuracy for k = 1 with 175000 training points is 67.524%\n",
      "Accuracy for k = 10 with 25000 training points is 73.232%\n",
      "Accuracy for k = 10 with 50000 training points is 73.98%\n",
      "Accuracy for k = 10 with 75000 training points is 73.216%\n",
      "Accuracy for k = 10 with 100000 training points is 73.196%\n",
      "Accuracy for k = 10 with 125000 training points is 73.8%\n",
      "Accuracy for k = 10 with 150000 training points is 73.756%\n",
      "Accuracy for k = 10 with 175000 training points is 72.136%\n",
      "Accuracy for k = 100 with 25000 training points is 77.78%\n",
      "Accuracy for k = 100 with 50000 training points is 77.944%\n",
      "Accuracy for k = 100 with 75000 training points is 77.932%\n",
      "Accuracy for k = 100 with 100000 training points is 77.932%\n",
      "Accuracy for k = 100 with 125000 training points is 77.896%\n",
      "Accuracy for k = 100 with 150000 training points is 77.932%\n",
      "Accuracy for k = 100 with 175000 training points is 77.908%\n"
     ]
    }
   ],
   "source": [
    "############# PART C ##################\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# implement k nearest neighbors algorithm using sklearn.neighbors.KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = [1, 10, 100]\n",
    "for i in k:\n",
    "    for j in range(25000,200000,25000):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        knn.fit(trees_training.iloc[:j,:-3], trees_training.iloc[:j,-3:])\n",
    "        print('Accuracy for k = ' + str(i) + ' with ' + str(j) + ' training points is ' + str(np.round(100*knn.score(trees_validation.iloc[:,:-3], trees_validation.iloc[:,-3:]),3)) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C Comments\n",
    "Clearly, the scikit-learn implementation of the KNN algorithm is WAY faster than my algorithm. This algorithm was able to run 20 times with more training points than my algorithm in the same amount of time it took my algorithm to run just once. There's a few things I could do to improve the speed of my algorithm:\n",
    "1. Use 'csr_matrix' from scikit-learn to turn the training data into a sparse representation because most of the features are 0\n",
    "2. Use an algorithm like 'Ball Tree' or 'K-D Tree' to find the nearest neighbors much faster. The scikit implementation of KNN uses one of these algorithms which decreases the accuracy of the results slightly but speeds up the search by more than an order of magnitude."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Spam Text Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# read data\n",
    "messages = pd.read_csv('HW5a_ham_spam.csv', sep=',', names = ['category','message'], header = None, skiprows = 1)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexh\\AppData\\Local\\Temp\\ipykernel_25868\\439784917.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  messages['message'] = messages['message'].str.replace('[^a-zA-Z0-9]', ' ')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>nah i don t think he go to usf he life around ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                            message\n",
       "0         1  go until jurong point crazy available only in ...\n",
       "1         1                            ok lar joking wif u oni\n",
       "2        -1  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3         1        u dun say so early hor u c already then say\n",
       "4         1  nah i don t think he go to usf he life around ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# PART A ##################\n",
    "\n",
    "# remove all non-alphanumeric characters from messages\n",
    "messages['message'] = messages['message'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "# convert all messages to lowercase\n",
    "messages['message'] = messages['message'].str.lower()\n",
    "\n",
    "# word tokenize messages for lemmatizer\n",
    "messages['message'] = messages['message'].apply(nltk.word_tokenize)\n",
    "\n",
    "# lemmatize messages\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "messages['message'] = messages['message'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x]))\n",
    "\n",
    "# word tokenize messages again\n",
    "# messages['message'] = messages['message'].apply(nltk.word_tokenize)\n",
    "\n",
    "# replace 'ham' with 1 and 'spam' with -1\n",
    "messages['category'] = messages['category'].map({'ham': 1, 'spam': -1})\n",
    "\n",
    "messages.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A Comments\n",
    "Tokenization separates each sentence into individual words, and lemmatization reduces words, usually verbs, to their base form. Tokenization allows us to analyze the data easier by picking out individual words rather than looking at whole complicated sentences. Lemmatization reduces noise in the data by combining words that essentially have the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "############# PART B ##################\n",
    "\n",
    "# generate multinomial features based on the number of occurrences of a set of words in the message using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# split messages into training and validation sets\n",
    "validation_data_length = 500\n",
    "messages_training = messages.iloc[:-validation_data_length,:]\n",
    "messages_validation = messages.iloc[-validation_data_length:,:]\n",
    "\n",
    "# look at only most frequenct m words\n",
    "m = [5, 10, 100, 500, 1000, 2500]\n",
    "\n",
    "# initalize empty lists to hold feature names and count features\n",
    "training_names = []\n",
    "training_features = []\n",
    "vali_names = []\n",
    "vali_features = []\n",
    "\n",
    "# apply CountVectorizer to messages\n",
    "for i in m:\n",
    "    count_vectorizer = CountVectorizer(stop_words='english', max_features=i)\n",
    "\n",
    "    # get count features for vali data\n",
    "    training_features_temp = count_vectorizer.fit_transform(messages_training['message'])\n",
    "\n",
    "    # convert matrix to array\n",
    "    training_features_temp = training_features_temp.toarray()\n",
    "\n",
    "    # append count features and feature names to lists\n",
    "    training_features.append(training_features_temp)\n",
    "    training_names.append(count_vectorizer.get_feature_names())\n",
    "\n",
    "for i in m:\n",
    "    count_vectorizer = CountVectorizer(stop_words='english', max_features=i)\n",
    "\n",
    "    # get count features for vali data\n",
    "    vali_features_temp = count_vectorizer.fit_transform(messages_training['message'])\n",
    "\n",
    "    # convert matrix to array\n",
    "    vali_features_temp = vali_features_temp.toarray()\n",
    "\n",
    "    # append count features and feature names to lists\n",
    "    vali_features.append(vali_features_temp)\n",
    "    vali_names.append(count_vectorizer.get_feature_names())\n",
    "    \n",
    "\n",
    "# now we have a list of count features and feature names for each value of m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# PART C ##################\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from cvxopt import matrix as cvxopt_matrix\n",
    "# from cvxopt import solvers as cvxopt_solvers\n",
    "\n",
    "# # implement soft-margin SVM with c = 0.1 to classify the 6 datasets\n",
    "# c = 0.1\n",
    "\n",
    "# # the following 3 functions were modified from homework 4\n",
    "\n",
    "# # using cvxopt to compute lambdas\n",
    "# def compute_lams(x,y):\n",
    "\n",
    "#     rows = x.shape[0]\n",
    "#     y = y.reshape(-1,1)*1. # reshape and cast to float for cvxopt\n",
    "\n",
    "#     # create D matrix\n",
    "#     mat = y*x\n",
    "#     D = np.dot(mat,mat.T)\n",
    "\n",
    "#     # set up cvxopt, using cvxopt notation\n",
    "#     P = cvxopt_matrix(D)\n",
    "#     q = cvxopt_matrix(-np.ones(rows))\n",
    "#     G = cvxopt_matrix(-np.eye(rows,rows))\n",
    "#     h = cvxopt_matrix(np.zeros(rows))\n",
    "#     A = cvxopt_matrix(y.reshape(1,-1))\n",
    "#     b = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "#     # run solver\n",
    "#     cvxopt_solvers.options['show_progress'] = False\n",
    "#     sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "#     lam = np.array(sol['x'])\n",
    "\n",
    "#     # ensure lams that should be 0 are actually 0\n",
    "#     lam[lam<0.001] = 0\n",
    "\n",
    "#     return lam\n",
    "\n",
    "# def compute_wstar(lam,x,y):\n",
    "#     # lam: vector of lam values computed using compute_lams\n",
    "#     # x: data you want to separate \n",
    "#     # y: labels for x \n",
    "\n",
    "#     wstar = np.zeros(x.shape[1])\n",
    "#     for j in range(0, x.shape[1]):\n",
    "#         for i in range(0, x.shape[0]):\n",
    "#             wstar[j] += lam[i]*y[i]*x[i][j]\n",
    "\n",
    "#     # compute geometric margin, w0star\n",
    "#     k = (lam > 0.001).flatten()\n",
    "    \n",
    "#     w0star = y[k] - np.dot(x[k],wstar)\n",
    "\n",
    "#     return wstar,w0star\n",
    "\n",
    "# # predictor\n",
    "# def predictor(lam,trainingdata,testingdata,w0star,ytrain):\n",
    "#     xtrain = trainingdata\n",
    "#     xtest = testingdata\n",
    "\n",
    "#     # get nonzero values for lam\n",
    "#     k = lam.nonzero()\n",
    "#     k = np.array(k)[0]\n",
    "\n",
    "#     # create vector of labels\n",
    "#     y = np.zeros(xtest.shape[0])\n",
    "#     for i in range(0,y.shape[0]):\n",
    "#         sum = 0\n",
    "#         for j in k:\n",
    "#             sum += lam[j]*ytrain[j]*xtrain[j]\n",
    "\n",
    "#         val = (np.dot(sum,xtest[i]) + w0star)[0]\n",
    "#         if val > 0:\n",
    "#             y[i] = 1\n",
    "#         else:\n",
    "#             y[i] = -1\n",
    "\n",
    "#     return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# PART C ##################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "\n",
    "# the following 3 functions were modified from homework 4\n",
    "\n",
    "def compute_lams(x,y,c):\n",
    "    rows = x.shape[0]\n",
    "    y = y.reshape(-1,1)*1. # reshape and cast to float for cvxopt\n",
    "\n",
    "    # create D matrix\n",
    "    mat = y*x\n",
    "    D = np.dot(mat,mat.T)\n",
    "\n",
    "    # set up cvxopt, using cvxopt notation\n",
    "    P = cvxopt_matrix(D)\n",
    "    q = cvxopt_matrix(-np.ones(rows))\n",
    "    G = cvxopt_matrix(np.vstack((-np.eye(rows), np.eye(rows))))\n",
    "    h = cvxopt_matrix(np.hstack((np.zeros(rows), c*np.ones(rows))))\n",
    "    A = cvxopt_matrix(y.reshape(1,-1))\n",
    "    b = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "    # run solver\n",
    "    cvxopt_solvers.options['show_progress'] = False\n",
    "    sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "    lam = np.array(sol['x'])\n",
    "\n",
    "    return lam\n",
    "\n",
    "\n",
    "def compute_wstar(lam,x,y):\n",
    "    # lam: vector of lam values computed using compute_lams\n",
    "    # x: data you want to separate \n",
    "    # y: labels for x \n",
    "\n",
    "    wstar = np.zeros(x.shape[1])\n",
    "    for j in range(0, x.shape[1]):\n",
    "        for i in range(0, x.shape[0]):\n",
    "            wstar[j] += lam[i]*y[i]*x[i][j]\n",
    "\n",
    "    # compute geometric margin, w0star\n",
    "    k = (lam > 0.001).flatten()\n",
    "    \n",
    "    w0star = y[k] - np.dot(x[k],wstar)\n",
    "\n",
    "    return wstar,w0star\n",
    "\n",
    "\n",
    "# predictor\n",
    "def predictor(lam,trainingdata,testingdata,w0star,ytrain):\n",
    "    xtrain = trainingdata\n",
    "    xtest = testingdata\n",
    "\n",
    "    # get nonzero values for lam\n",
    "    k = lam.nonzero()\n",
    "    k = np.array(k)[0]\n",
    "\n",
    "    # create vector of labels\n",
    "    y = np.zeros(xtest.shape[0])\n",
    "    for i in range(0,y.shape[0]):\n",
    "        sum = 0\n",
    "        for j in k:\n",
    "            sum += lam[j]*ytrain[j]*xtrain[j]\n",
    "\n",
    "        val = (np.dot(sum,xtest[i]) + w0star)[0]\n",
    "        if val > 0:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = -1\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'A' must be a 'd' matrix with 5072 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25868\\4246327510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# find separator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mlam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_lams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mwstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_wstar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25868\\2228377646.py\u001b[0m in \u001b[0;36mcompute_lams\u001b[1;34m(x, y, c)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# run solver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mcvxopt_solvers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'show_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0msol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvxopt_solvers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mlam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexh\\anaconda3\\lib\\site-packages\\cvxopt\\coneprog.py\u001b[0m in \u001b[0;36mqp\u001b[1;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[0;32m   4483\u001b[0m             'residual as dual infeasibility certificate': dinfres}\n\u001b[0;32m   4484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4485\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconeqp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkktsolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alexh\\anaconda3\\lib\\site-packages\\cvxopt\\coneprog.py\u001b[0m in \u001b[0;36mconeqp\u001b[1;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[0;32m   1911\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmatrixA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypecode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'd'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m             raise TypeError(\"'A' must be a 'd' matrix with %d columns\" \\\n\u001b[0m\u001b[0;32m   1914\u001b[0m                 %q.size[0])\n\u001b[0;32m   1915\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'A' must be a 'd' matrix with 5072 columns"
     ]
    }
   ],
   "source": [
    "# get training and validation labels as vectors\n",
    "training_labels = messages_training['category'].values\n",
    "vali_labels = messages_validation['category'].values\n",
    "\n",
    "# implement soft-margin SVM with c = 0.1 to classify the 6 datasets\n",
    "c = 0.1\n",
    "\n",
    "\n",
    "# for i in range(0,len(m)):\n",
    "for i in range(0,1):\n",
    "    # find separator\n",
    "    lam = compute_lams(training_features[i],training_labels[i],c)\n",
    "    wstar, w0star = compute_wstar(lam,training_features[i],training_labels[i])\n",
    "\n",
    "    wstar = np.insert(wstar,0,1)\n",
    "    print(f\"wstar = {wstar}\")\n",
    "\n",
    "    # geometric margin\n",
    "    # print(f\"geometric margin = {w0star[0]}\")\n",
    "\n",
    "    # predictor and validation loss\n",
    "    y = predictor(lam,training_features[i],vali_features[i],w0star,vali_labels[i])\n",
    "    y = np.int0(y)\n",
    "\n",
    "    n = vali_features[i].shape[0]\n",
    "    vali_loss = 0\n",
    "    for j in range(validation_data_length,n):\n",
    "        vali_loss += (y[j-validation_data_length]-vali_labels[i][j])**2\n",
    "    vali_loss *= n**(-1)\n",
    "    print(f\"validation loss = {vali_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
